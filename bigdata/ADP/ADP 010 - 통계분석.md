# ADP 010 - 통계분석

# 01 통계학 개론

## 1. 통계분석

### 표본추출 방법

* 1) 확률적 추출
  * 단순 무작위추출 : 모집단 개체 N개 중 임의로 표본 수 n개를 선택
  * 계통추출 : 모집단 개채에 일련번호를 부여한 뒤, 첫 번째 표본 선택한 후 일정 간격따라 다음 표본 선택
  * 층화추출 : 모집단을 집단, 층(strata)으로 나누고, 각 집단 내에서 표본 n개를 무작위 추출한다.
  * 군집추출 : 모집단을 집단, 클러스터(cluster)로 나누고, 집단 몇 개를 선택한 후 표본 n개를 추출한다.

* 2) 비확률적 추출
  * 판단추출: 전문가가 표본 선택
  * 할당추출: 모집단을 여러 집단으로 나누고, 연구 판단에 따라 선택
  * 편의추출: 연구자가 쉽게 접근할 수 있는 표본 추출



### 자료의 종류

* 명목척도(nominal scale) : 분류 목적으로 숫자를 부여
* 서열척도(ordinal scale) : 높낮음이 있어 순위 제공, 양적비교 불가
* 등간척도(interval scale) : 순위 사이 간격이 동일하여 양적비교 가능, 절대 0점 존재하지 않는다. (온도계, 물가지수, Likert 척도)
* 비율척도(ratio scale) : 절대 0점이 존재하여 측정값 사이 비율계산이 가능

---

## 3. 확률, 확률분포

* 표본공간에 발생하는 원소를 정의역, 이에 대응되는 실수값을 치역으로 하는 함수를 확률변수, 치역에 해당하는 실숫값을 확률로 나타낸 것을 확률분포라 한다.
* 결합확률분포: 두 확률변수 X, Y의 모든 값과 이에 대응하는 확률을 표나 그림으로 나타낸 것
  * 이산확률분포 : 베르누이분포, 이항분포, 기하분포, 다항분포, 포아송분포
  * 연속확률분포 : 균일분포, 정규분포(카이제곱분포, F분포), 지수분포



---

## 4. 추정과 가설검정

> 좋은 추정량이 되는 조건:
>
> * 불편성: 추정량 기댓값이 모수 값과 같아야 한다.
> * 효율성: 추정량의 분산이 작아야 한다.
> * 충족성: 표본자료에 내재된 모든 정보를 활용할 수 있도록 정의된 추정량. 효율성의 필요조건.
> * 일관성: 표본 크기가 커지면 표본오차가 작아져야 한다.

* 점추정 : 모수가 특정한 값일 것이라 추정
* 구간추정 : 일정한 크기의 신뢰수준으로 모수가 특정 구간에 있을 것을 추정
* 가설검정 : 표본을 이용하여 미지의 모집단 모수에 대한 두 가지 가설, 즉 대립가설(연구자가 입증하려는 가설), 귀무가설(=영가설, 기존에 받아들여지던 가설)을 놓고 통계적으로 의사결정하는 것.



> * 제 1종 오류($\alpha$ error) : 귀무가설이 옳은데도 기각하는 오류. = 음성인 것을 양성으로 판단하는 오류(거짓양성, false positive)
> * 제 2종 오류($\beta$ error) : 귀무가설이 옳지 않은데도 채택하는 오류 = 양성인 것을 음성으로 판단하는 오류(거짓음성, false negative)
>
> 두 오류는 trade-off 관계이므로 제 1종 오류의 최대 허용치를 정하고, 제 2종 오류가 낮아지는 검정 방법을 택하기도 한다.



> * 검정통계량(test statistics) : **관찰된 표본으로부터 구하는 통계량**으로 분포가 가설에서 주어지는 모수에 의존한다. 검정 시 가설 진위를 판단하는 수단이 된다.
> * 유의확률(significance probability) = p-value : 대립가설을 더 지지하는 검정통계량이 나올 확률
> * 유의수준(significance level) : 제 1종 오류 확률의 최댓값
> * 기각역(critical region) : 검정통계량 분포에서 유의수준 $\alpha$ 크기에 해당하는 영역으로, 계산된 검정통계량의 유의성을 판정하는 기준



---

## 5. 비모수적 검정

* 모집단 분포에 대해 아무 제약을 두지 않고 검정한다. 관측된 자료가 특정 분포를 따른다고 가정할 수 없는 경우다.
* 가설은 단지 분포가 동일하다, 동일하지 않다 정도의 형태만 설정한다.
* 관측값들의 순위나, 관측값 사이 부호 등을 통해 검정한다.



### 베이지안 추론

* 모수를 상수가 아닌 확률변수로 보고, 사후분포(posterior distribution)를 유도한다.



---

# 02 기초 통계분석

## 1. 기술통계

모집단 혹은 표본으로부터 얻은 데이터에 대한 숫자, 그래프 요약을 통해 정리한다.

<br />

## 2. 회귀분석

### 1) 단순회귀, 중귀회귀 분석

### 2) 선형회귀모형

* X와 Y가 1차식
* 독립 변수 X에 대한 Y의 확률분포가 존재한다.

### 3) 단순회귀모형

$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
$$

* $\epsilon$은 오차항, 독립적이며 $N(0, \sigma^2)$ 분포를 이룬다.

### 4) 회귀 모형에 대한 가정

* **선형성 :** 독립변수가 변함에 따라 종속변수가 선형적으로 변한다.
* **독립성** : **잔차**와 독립변수 간 관련이 없다.
* **등분산성 :** 오차항 분포는 동일한 분산을 가진다.(선형회귀에서는 오차항 분포는 평균이 0이고, 분산이 일정한 정규분포를 따른다.)
* **비상관성** : **잔차**들끼리 상관이 없다.
* **정상성** : **잔차**항이 정규분포를 이룬다.

> * 에러(error) : “모집단”에서 회귀식을 통해 얻은 예측값과 실제 관측값의 차이
> * 잔차(residual) : “표본집단”에서 회귀식을 통해 얻은 예측값과 실제 관측값의 차이

 <br />

> 회귀분석 모형에서 확인해야할 사항
>
> * 통계적 유의미성 : F분포, p-value 확인
> * 회귀계수 유의미성 : 회귀계수 t값, p-value 확인
> * 모형의 설명력 : 결정 계수 확인
> * 모형이 데이터를 잘 적합하는지 : 잔차통계량 확인 후 회귀진단



### 실습

```python
# 단순 선형회귀 분석
set.seed(2)
x = runif(10, 0, 11)
y = 2 + 3*x + rnorm(10, 0, 2)
dfrm <- data.frame(x, y)
lm(y~x, data=dfrm)

Call:
lm(formula = y ~ x, data = dfrm)

Coefficients:
(Intercept)            x  
      3.406        2.822 
```



```python
# 다중 선형회귀 분석
set.seed(2)
u = runif(10, 0, 11)
v = runif(10, 11, 20)
w = runif(10, 1, 30)
y = 3 + 0.1*u + 2*v - 3*w + rnorm(10, 0, 2)

dfrm <- data.frame(y, u, w, v)
lm(y~u+v+w, data=dfrm)
```



```python
# 회귀모델 적합성 검증(모델평가)
> m <- lm(y~u+v+w, data=dfrm)
> summary(m)

Call:
lm(formula = y ~ u + v + w, data = dfrm)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.2241 -0.9711  0.2228  1.2309  2.3316 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -5.25066    6.92959  -0.758   0.4773    
u           -0.08627    0.20092  -0.429   0.6826    
v            2.77172    0.48140   5.758   0.0012 ** 
w           -3.22203    0.09442 -34.125 4.22e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.099 on 6 degrees of freedom
Multiple R-squared:  0.9957,	Adjusted R-squared:  0.9935 
F-statistic: 463.1 on 3 and 6 DF,  p-value: 1.736e-07
```

* `Residual standard error: 2.099 on 6 degrees of freedom` : 잔차 값이 작으므로 데이터를 **잘 적합한다.**
* `Multiple R-squared:  0.9957,	Adjusted R-squared:  0.9935` : 결정계수, 수정된 결정계수 모두 1에 가까우므로 회귀식이 데이터를 매우 **잘 설명한다.**
* `F-statistic: 463.1 on 3 and 6 DF,  p-value: 1.736e-07` : F 통계량, p-value < 0.05 이므로 회귀모형이 **통계적으로 유의미하다.**



```R
# 모델 진단 그래프
data(cars)
m <- lm(dist~speed, cars)
par(mfrow=c(2, 2))
plot(m)
```



> 다중공선성(Multicollinearity)
>
> * 모형의 일부 예측변수가 다른 예측변수와 상관성이 있을 경우.
> * 다중공선성은 회귀계수 분산을 증가시켜 불안정하고 해석하기 어렵게 만든다.
> * `vif()` 함수 사용, `VIF` > 10 이면 다중공선성 존재.



### 5) 최적 회귀방정식 선택: 설명변수 선택

* y에 영향을 미치는 모든 설명변수 x를 선택하되,
* 관리하기 위해 가능한 적은 수 설명변수만 포함시킨다.



### 6) 설명변수 선택

* 1) 모든 가능한 조합의 회귀분석
  * 적합한 회귀모형을 고를 때, 결정계수(R-square) 외에도 AIC, BIC 기준으로 고른다. 값이 작을수록 모델 적합성이 좋다.
* 2) 단계적 변수선택
  * 단계별 선택 : 모든 변수에서 시작해서 도움 되는 변수 삭제 또는 기준 통계치를 개선키는 변수 추가.
  * 후진제거법 : 모든 변수에서 시작해서 기준 통계치에 도움이 안되는 변수부터 제거
  * 전진선택법 : 절편에서 시작해서 기준 통계치를 가장 많이 개선시키는 변수 추가

단계별 변수선택은 `step()` 함수로도 가능.

> 회귀모형이 최적 모델임을 아는 지표
>
> * 결정계수 1에 가까울수록
> * 회귀계수 0일 확률에 대한 판단
> * AIC(아케이케), BIC(슈바르츠 통계량), Cp(멜로우스 통계량) 낮을수록 최적ㄱ모델



---

## 3. 정규화 선형회귀

### 1) 정규화(Regularization)

* MSE 기댓값 = error + $Bias^2$ + Variance 로 Bias, Variance 둘 다 감소하면 좋다. 하지만 이 둘은 trade-off 관계이다.
* MSE를 최소로 해야 한다. train data를 잘 설명하고, test  data 예측 성능이 우수하다.



$L(\beta) = \underset{\beta}{\text{min}} \sum_{i=1}(y_i - \hat{y})^2 + \lambda \sum_{i=1}^p \beta_i^2$

* 베타계수에 패널티를 부여하여 모델에 변화를 준다.(정규화)
* 람다는 정규화 모형을 조정하는 하이퍼 파라미터



| 릿지(능형) 회귀                               | 라쏘(Lasso) 회귀                  | 엘라스틱넷(ElasticNet)                                   |
| --------------------------------------------- | --------------------------------- | -------------------------------------------------------- |
| L2 norm 정규화                                | L1 norm 정규화                    | L2 + L1 norm 정규화                                      |
| 변수 선택 불가능                              | 변수 선택 가능                    | 변수 선택 가능                                           |
| 변수간 상관관계 높은상황에서 예측성능이 좋다. | 릿지에 비해 예측 성능이 떨어진다. | 상관관계 큰 변수를 동시에 선택하고 배제하는 특성이 있다. |

