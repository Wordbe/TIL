# Spark 01

# 1 스파크 구조

스파크는 인메모리 기반 대용량 데이터 고속 처리 엔진이며, 범용 분산 클러스터 컴퓨팅 프레임워크 입니다.



스파크는 2011년 버클리 대학의 AMPLab에서 개발되어, 현재 아파치 재단의 오픈소스로 관리되고 있습니다.

2014년 정식 출시, 2020년 3월 현재 2.4.5 RELEASE 버전이 출시 되있는 상태입니다.

<br>

## 특징

1) Speed

* In-Memory 기반 빠른 처리

2) Ease of Use

* 다양한 언어 지원(Java, Scala, Python, R, SQL)을 통한 사용의 편의성

3) Generality

* SQL, Streaming, 머신러닝, 그래프 연산 등 다양한 컴포넌트 제공

4) Run Everywhere

* YARN, Mesos, Kubernetes 등 다양한 클러스터에서 동작 가능
* HDFS, Casandra, Hbase 등 다양한 파일 포맷 지원

<br>

---

### 1) 인메모리 기반의 빠른 처리

스파크는 인메모리 기반의 처리로 **맵리듀스 작업처리에 비해 디스크는 10배, 메모리 작업은 100배 빠른 속도**를 가지고 있습니다. 맵리듀스는 작업의 중간 결과를 디스크에 쓰기 때문에 IO로 인하여 작업 속도에 제약이 생깁니다. **스파크는 메모리에 중간 결과를 메모리에 저장하여 반복 작업의 처리 효율이 높습니다.**

![spark inmemory](https://spark.apache.org/images/logistic-regression.png)

<br>

### 2) 다양한 컴포넌트 제공

스파크는 단일 시스템 내에서 스파크 스트리밍을 이용한 스트림 처리, 스파크 SQL을 이용한 SQL 처리, MLib 를 이용한 머신러닝 처리, GraphX를 이용한 그래프 프로세싱을 지원합니다. 추가적인 소프트웨어의 설치 없이도 다양한 애플리케이션을 구현할 수 있고, 각 컴포넌트간의 연계를 이용한 애플리케이션의 생성도 쉽게 구현할 수 있습니다.

<br>

### 3) 다양한 언어 지원

스파크는 자바, 스칼라, 파이썬, R 인터페이스등 다양한 언어를 지원하여 개발자에게 작업의 편의성을 제공합니다. 하지만 언어마다 처리하는 속도가 다릅니다. 따라서 **성능을 위해서 Scala로 개발을 진행하는 것을 권장**합니다.

<br>

### 4) 다양한 클러스터 매니저 지원

클러스터 매니저로 YARN, Mesos, Kubernetes, standalone 등 다양한 포맷을 지원하여 운영 시스템 선택에 다양성을 제공합니다. 또한, HDFS, 카산드라, HBase, S3 등의 다양한 데이터 포맷을 지원하여 여러 시스템에 적용이 가능합니다.

<br>

### 5) 다양한 파일 포맷 지원 및 Hbase, Hive 등과 연동 가능

스파크는 기본적으로 TXT, Json, ORC, Parquet 등의 파일 포맷을 지원합니다. S3, HDFS 등의 파일 시스템과 연동도 가능하고, HBase, Hive 와도 간단하게 연동할 수 있습니다.

<br>

## 컴포넌트 구성

스파크 컴포넌트 구성은 **스파크 라이브러리, 스파크 코어, 클러스터 매니저**로 구분되어 있습니다.

![spark](http://cfile25.uf.tistory.com/image/2140BE3C555DFB51305898)

<br>

### 스파크 코어

**Spark Core는 메인 컴포넌트로 작업 스케줄링, 메모리 관리, 장애 복구와 같은 기본적인 기능**을 제공하고, RDD(Resilient Distributed Dataset), Dateset, DataFrame을 이용한 스파크 연산을 처리합니다.

<br>

### 스파크 라이브러리

스파크 라이브러리는 **빅데이터 처리를 위한 작업용 라이브러리**입니다. 스파크의 공식 지원 라이브러리는 다음과 같습니다.

<br>

#### 1 Spark SQL

스파크 SQL은 **SQL을 이용하여 RDD, DataSet, DataFrame 작업을 생성하고 처리합**니다. 하이브 메타스토어와 연결하여 하이브의 메타 정보를 이용하여 SQL 작업을 처리할 수 있습니다. 샤크(Shark)는 하이브에서 스파크 작업을 처리할 수 있도록 개발하는 외부 프로젝트 였는데 현재는 스파크 SQL로 통합되었습니다.

#### 2 Spark Streaming

스파크 스트리밍은 실시간 데이터 스트림을 처리하는 컴포넌트입니다. 스트림 데이터를 작은 사이즈로 쪼개어                                                                                                                                                                                                              RDD 처럼 처리합니다.

#### 3 MLlib

MLlib는 스파크 기반의 머신러닝 기능을 제공하는 컴포넌트입니다. 분류(classification), 회귀(regression), 클러스터링(clustering), 협업 필터링(collaborative filtering) 등의 머신러닝 알고리즘과 모델 평가 및 외부 데이터 불러오기 같은 기능도 지원합니다.

#### 4 GraphX

GraphX는 **분산형 그래프 프로세싱이 가능하게 해주는 컴포넌트**입니다. 각 간선이나 점에 임의의 속성을 추가한 지향성 그래프를 만들 수 있습니다.

## 클러스터 매니저

**스파크 작업을 운영하는 클러스터 관리자** 입니다. 스파크는 다양한 클러스터 매니저를 지원합니다. 스파크에서 제공하는 **스탠드얼론(Standalone) 관리자**를 이용할 수도 있고, **메조스(Mesos), 얀(YARN), 큐버네티스(Kubernetes) 등의 관리자를 지원**합니다.

<br>

---

# 2 스파크 모니터링

스파크 작업을 모니터링 하는 방법은 웹UI 또는 REST API를 이용하는 방법이 있습니다.

- 스파크 컨텍스트 웹UI
  - **스파크 컨텍스트가 실행되면 4040포트로 웹UI를 실행**
  - 하나의 노드에 여러개의 컨텍스트가 실행되면 포트번호가 1씩 증가하면서 생성(4041, 4042, ...).
  - 스파크 컨텍스트가 실행되고 있는 동안에만 사용 가능
- 스파크 히스토리 서버
  - 실행중인 작업과 실행이 끝난 작업의 히스토리를 확인하기 위한 서버
  - `start-history-server.sh`로 실행하고 기본 접속 포트는 `18080`
  - 영구적인 저장소에 스파크 작업 내역을 저장 하고 사용자의 요청에 정보를 반환

## 확인 가능 정보

- 스테이지와 태스크 목록
- RDD 크기와 메모리 사용량
- 환경변수 정보
- 익스큐터의 정보



## REST API

스파크 히스토리 서버는 작업 애플리케이션에 대해 REST API를 이용해 json 형태의 정보를 제공합니다.

```shell
curl -s http://$(hostname -f):18080/api/v1/applications
curl -s http://$(hostname -f):18080/api/v1/[app-id]/jobs
```







---

**Reference**

https://wikidocs.net/26630

https://wikidocs.net/72340