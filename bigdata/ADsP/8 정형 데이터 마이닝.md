# 8 정형 데이터 마이닝

# 01 데이터 마이닝 개요

1) 분류

2) 추정(Estimation) : 연속된 변수 값을 추정한다. ex) 신경망 모형

3) 예측(Prediction) : 미래양상 예측, 추정 (=분류, 추정)

4) 연관분석(Association Analysis) : 장바구니 분석

5) 군집(Clustering) : 데이터 마이닝이나 모델링의 준비 단계로 사용한다.

6) 기술(Description) : 데이터가 가진 의미를 기술한다. 



데이터마이닝 5단계

목적 정의 → 데이터 준비 → 데이터 가공 → 데이터 마이닝 기법 적용 → 검증



# 02 분류 분석

## 1 로지스틱 회귀모형



* 로지스틱 회귀계수는 오즈(Odds)로 나타낸다.

$$
오즈 = 성공률 / 실패율 = P_i / (1 - P_i)
$$



* 오즈와 단순확률(p)는 거의 같다.

* 오즈비(Odds ratio) : 오즈를 오즈로 나누면 비교가 가능하다. 오즈1 / 오즈2

* 로짓(logit) = log(오즈비)
  $$
  0 \leq p \leq 1
  $$
  
  $$
  -\infty \leq log(\frac{p}{1-p}) \leq \infty
  $$

* 로짓변환을 통해, 이산적인 종속변수를 연속형변수로 바꿔 분석이 가능하다. → OLS가 가능하다.



### 1) 선형회귀 vs 로지스틱 회귀분석

|                | 일반선형 회귀분석 | 로지스틱 회귀분석          |
| -------------- | ----------------- | -------------------------- |
| 종속변수       | 연속형 변수       | 이산형 변수                |
| 모형 탐색 방법 | 최소자승법        | 최대우도법, 가중최소자승법 |
| 모형 검정      | F-tset, t-test    | $\chi^2$ test              |



> 시그모이드 함수
> $$
> \frac{1}{1  + e^{-x}}
> $$



```R
x, male, female
total = male + female
pmale <- male / total
z <- lm(pmale~x)
summary(z)

p <- coefficients(z)[1] + coefficients(z)[2] * x
```



* 로짓 변환 :  로지스틱 회귀모형은 적절한 변환을 통해 곡선을 직선 형태로 바꿀 수 있다

```R
logit <- log(pmale / (1-pmale))
z1 <- lm(logit~x)
summary(z1)

logit2 <- coefficients(z1)[1] + coefficients(z1)[2] * x
rmalehat <- exp(logit2) / (1 + exp(logit2))
rmalehat
```



* 최대우도추정법을 사용하여 추정할 수 있다.

  관측값들이 가정된 모집단에서, **표본으로 추출될 가능성이 가장 크게 되도록 하는 회귀계수를 추정**하는 방법이다.

  표본의 수가 클수록 최대우도추정법은 안정적이다.

  로지스틱 회귀 추정에서 최소제곱법을 이용하여 얻은 회귀계수 값들은 불안정한 것으로 알려져 있으므로, 최대우도추정을 이용해 회귀방정식을 추정한다.

```R
logit <- glm(pmale~x, family="binomial", weights=total)
summary(logit)
```

로지스틱 회귀분석



다중 로지스틱 회귀분석

```R
str(mtcars)

glm.vs <- glm(vs~mpg+am, data=mtcars, family = "binomial")
summary(glm.vs)
Call:
glm(formula = vs ~ mpg + am, family = "binomial", data = mtcars)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.05888  -0.44544  -0.08765   0.33335   1.68405  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)   
(Intercept) -12.7051     4.6252  -2.747  0.00602 **
mpg           0.6809     0.2524   2.698  0.00697 **
am           -3.0073     1.5995  -1.880  0.06009 . 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 43.860  on 31  degrees of freedom
Residual deviance: 20.646  on 29  degrees of freedom
AIC: 26.646

Number of Fisher Scoring iterations: 6
```



---

**ANOVA 결과**, Null deviance와 Residual deviance의 차이는, Nul 모델과 비교하여 해당 변수가 모델에 포함되었을 때 성능이 나아진 수치를 보여준다.

`anova()` 함수는 모형의 적합 단계별로 이탈도의 감소량과, 유의성 검정 결과를 제시해준다.

```R
anova(glm.vs, test="Chisq")
```



선형회귀 분석에서의 $R^2$ 와 유사한 개념인, Mcfadden $R^2$ 로 모델 fit을 확인할 수 있다.

```R
install.packages("pscl")
library(pscl)
pR2(glm.vs)
```





## 2 신경망 모형

### 1) 인공신경망

### 2) 가중치 조절 작업

### 3) 은닉층, 은닉 노드 수 고려 사항

### 4) 신경망 모형 장점

### 5) 신경망 모형 단점

### 6) 경사하강법

### 7) 기울기 소실문제(Vanishing Gradient Problem)



## 3 의사 결정나무 모형

### 1) 의사결정나무

### 2) 데이터 분할과 과대적합

### 3) 의사결정나무 구분

**분류나무 (Classification Tree)**

**회귀나무 (Regression Tree)**



## 4 앙상블 모형

### 1) 배깅(Bagging)

### 2) 부스팅(Boosting)

### 3) 랜덤포레스트(Random Forest)



## 5 SVM, 서포트 벡터 머신



## 6 나이브 베이즈 분류모형(Naive Bayes Classification)



## 7 K-NN (K-Nearest Neighbor)



## 8 모형평가

### 1) 홀드아웃(Hold-out) 방법

### 2) 교차검증(Cross Validation)

### 3) 붓스트랩

**오분포표(confusion matix)**

**ROC 그래프(Receiver Operating Characteristic)**

### 4) 이익도표와 향상도



---

# 03 군집분석

## 1 계층적 군집(Hierarchical Clustering)

* 계통도, 덴드로그램 등의 형태로 결과가 주어진다.
* 각 개체는 하나의 군집에만 속하게 된다.
* **응집형** 
  * 군집간 연결 방법으로 **단일(최단) 연결법, 완전(최장) 연결법, 평균연결법, 중심연결법, 와드연결법**이 있다.
  * 두 개체 간 거리에 기반하므로 거리 측정에 대한 정의가 필요하다.
* **분리형**
  * **다이아나 (DIANA) 방법**



### 1) 수학적 거리

* 유클리드
  $$
  d(I, j) = \sqrt{(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2 + \cdots + (x_{ip} - x_{jp})^2}
  $$
  방향성이 고려되지 않는다.

  

* 맨해튼 거리
  $$
  d(I, j) = \vert x_{i1} - x_{j1} \vert + \vert x_{i2} - x_{j2} \vert + \cdots + \vert x_{ip} - x_{jp} \vert
  $$
  
* 민코프스키(Minkowski) 거리

$$
d(i, j) = [ \sum_{k=1}^p \vert x_{ik} - x_{jk} \vert^m]^{1/m}
$$



### 2) 표준화 거리(통계적 거리)

각 변수를 각 표준편차의 척도 변환 후 유클리드 거리를 계산한 거리이다. 표준화를 하게 되면 척도(scale)의 차이, 분산의 차이로 인한 왜곡을 피할 수 있다. 통계적 거리(Statistical distance) 라고도 한다.

* 마할라노비스

$$
d_{ijj} = (x_{i} - x_{j})' s^{-1} (x_{i} - x_{j})
$$

변수의 표준화와 동시에 변수 간 상관성을 동시에 고려한 통계적 거리이다.

> * R에서 계층적 군집을 수행할 때 병합적 방법을 사용하는 함수에는 hclust, cluster 패키지의 `agnes()`, `mclust()` 함수가 있다. 
> * 분할적 방법을 사용하는 함수에는 cluster 패키지의 `diana()`, `mona()` 함수가 있다.

### 3) 계층적 군집의 특징

매 단계에서 지역적 최적화를 수행하므로, 결과가 반드시 전역적인 최적해라고 볼 수 없다.

병합적 방법에서 한 번 군집이 형성되면 다른 군집으로 이동할 수 없다.



> * 캔버라 거리 : 가충치 있는 맨해튼 거리
> * 체비셰프 거리 : 변숫값의 최대 차이의 절댓값 $D(x, y) = max_i \vert x_i = y_i \vert$
> * 코사인 유사도 : 두 벡터의 내적을 각 벡터의 크기로 나눈 값을 1에서 뺀 것
> * 자카드 계수 : 명목형 데이터에 대한 유사성 척도 (교집합 원소수 / 합집합 원소수)
> * 단순일치계수(Simple Matching Coefficient) = 단순 매칭 계수
> * 순위상관계수(Rank Correlation Coefficient)







## 2 비계층적 군집

### 1) k-평균 군집(k-means clustering)

원하는 군집 수만큼 초깃값 지정하고, 군집을 형성한 뒤, 군집의 평균을 재계산해서 초깃값을 갱신한다. 최종적으로 k개의 군집을 형성한다.

> 군집화가 잘되었는지 확인하는 척도
>
> 1) Dumm Index = 군집 간 거리 중 최솟값 / 군집 내 데이터들 거리 중 최댓값 : 분모 작을수록, 분자 클수록 Dumm Index 커지고 군집화가 잘 되었다고 볼 수 있다.
>
> 2) 실루엣(Silhouette) 지표 : 1인 경우 한 군집의 모든 개체가 모두 붙어있는 경우이다. 0.5보다 크면 군집 결과가 타당하다고 평가하고, 1에 가까울수록 군집화가 잘되었다고 본다.



## 3 혼합분포군집

모형기반(model-based)의 군집 방법이다.  데이터가 k개의 모수적 모형의 가중합으로 표현되는 모집단 모형으로 나왔다는 가정하에 **모수와 함께 가중치를 추정하는 방법**을 사용한다.

**EM 알고리즘**

1) K-means clustering 처럼 랜덤하게 초기화

**2) E-step : 각 자료가 어느 집단에 속하는지에 대한 정보를 가지는 변수(잠재변수, latent variable) 생성한다.**

**3) M-step : 최대우도추정으로 모수를 추정하고, 모수의 값이 수렴할 때까지 반복한다.**



```R
install.packages("mclust")
library(mclust)
mc <- Mclust(iris[, 1:4], G=3)
summary(mc, parameters=TRUE)
```





## 4 SOM(Self-Organizing Maps, 자기조직화지도)

### 1) SOM process

* 인공신경망의 한 종류이다.
* 차원축소, 군집화를 동시에 수행하는 방법이다.
* 입력 벡터를 훈련집합에서 매치되도록 가중치를 조정하는 뉴런 격자에 기초한 자율학습(Unsupervised Learning)의 한 방법이다.

> SOM 기능
>
> * 1) 구조탐색 : 데이터 특징을 파악하여 유사 데이터를 군집화한다.
> * 2) 차원축소, 시각화 : 통상 2차원 그리드에 매핑하여 인간이 시각적으로 인식할 수 있게 돕는다.

* 입력변수의 위치 관계를 그대로 보존하므로, 실제 데이터가 유사하면 지도상에 가깝게 표시된다.



### 2) SOM vs 신경망 모형



```R
install.packages("kohonen")
library(kohonen)
data("wines")
head(wines)
set.seed(7)

wine.som <- som(scale(wines), grid=somgrid(5, 4, "hexagonal")) # scale() 표준화함수
summary(wine.som)
plot(whine.som, main="wine data")

par(mfrow=c(1, 3))
plot(wine.som, type="counts")
plot(wine.som, type="quality")
plot(wine.som, type="mapping")
```





### 



## 5 밀도기반군집(Density-Based Clustering)

임의의 형태의 군집을 찾는데 장점이 있다.



**DBSCAN(Density-based Spatial Clutering of Application with Noise)**

1) eps, minnPts 설정 (2개 파라미터 필요)

2) 잡음점을 군집에서 제외

3) eps 반경 안에 있는 코어점들을 서로 연결

4) 연결된 코어점들을 하나의 군집으로 형성

5) 경계점은 관련된 코어점을 포함하는 군집 중 하나에 할당



```R
install.packages("fpc")
library(fpc)
data(iris)
iris1 <- iris[-5]

df <- dbscan(iris1, eps=0.42, MinPts=5)
table(df$cluster, iris$Species)
plot(df, iris1)
plotcluster(iris1, df$cluster)
```







---

# 04 연관분석

## 1 연관규칙

### 1) 연관규칙의 개념

### 2) 연관규칙의 측정 지표

* 지지도
* 신뢰도
* 향상도

### 3) 연관분석 절차



---

## 05 순차패턴 분석

