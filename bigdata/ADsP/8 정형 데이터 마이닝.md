# 8 정형 데이터 마이닝

# 01 데이터 마이닝 개요

1) 분류

2) 추정(Estimation) : 연속된 변수 값을 추정한다. ex) 신경망 모형

3) 예측(Prediction) : 미래양상 예측, 추정 (=분류, 추정)

4) 연관분석(Association Analysis) : 장바구니 분석

5) 군집(Clustering) : 데이터 마이닝이나 모델링의 준비 단계로 사용한다.

6) 기술(Description) : 데이터가 가진 의미를 기술한다. 



데이터마이닝 5단계

목적 정의 → 데이터 준비 → 데이터 가공 → 데이터 마이닝 기법 적용 → 검증



# 02 분류 분석

## 1 로지스틱 회귀모형

### 1) 선형회귀 vs 로지스틱 회귀분석



## 2 신경망 모형

### 1) 인공신경망

### 2) 가중치 조절 작업

### 3) 은닉층, 은닉 노드 수 고려 사항

### 4) 신경망 모형 장점

### 5) 신경망 모형 단점

### 6) 경사하강법

### 7) 기울기 소실문제(Vanishing Gradient Problem)



## 3 의사 결정나무 모형

### 1) 의사결정나무

### 2) 데이터 분할과 과대적합

### 3) 의사결정나무 구분

**분류나무 (Classification Tree)**

**회귀나무 (Regression Tree)**



## 4 앙상블 모형

### 1) 배깅(Bagging)

### 2) 부스팅(Boosting)

### 3) 랜덤포레스트(Random Forest)



## 5 SVM, 서포트 벡터 머신



## 6 나이브 베이즈 분류모형(Naive Bayes Classification)



## 7 K-NN (K-Nearest Neighbor)



## 8 모형평가

### 1) 홀드아웃(Hold-out) 방법

### 2) 교차검증(Cross Validation)

### 3) 붓스트랩

**오분포표(confusion matix)**

**ROC 그래프(Receiver Operating Characteristic)**

### 4) 이익도표와 향상도



---

# 03 군집분석

## 1 계층적 군집(Hierarchical Clustering)

* 계통도, 덴드로그램 등의 형태로 결과가 주어진다.
* 각 개체는 하나의 군집에만 속하게 된다.
* **응집형** 
  * 군집간 연결 방법으로 **단일(최단) 연결법, 완전(최장) 연결법, 평균연결법, 중심연결법, 와드연결법**이 있다.
  * 두 개체 간 거리에 기반하므로 거리 측정에 대한 정의가 필요하다.
* **분리형**
  * **다이아나 (DIANA) 방법**



### 1) 수학적 거리

* 유클리드
  $$
  d(I, j) = \sqrt{(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2 + \cdots + (x_{ip} - x_{jp})^2}
  $$
  방향성이 고려되지 않는다.

  

* 맨해튼 거리
  $$
  d(I, j) = \vert x_{i1} - x_{j1} \vert + \vert x_{i2} - x_{j2} \vert + \cdots + \vert x_{ip} - x_{jp} \vert
  $$
  

* 민코프스키(Minkowski) 거리

$$
d(i, j) = [ \sum_{k=1}^p \vert x_{ik} - x_{jk} \vert^m]^{1/m}
$$



### 2) 표준화 거리(통계적 거리)

각 변수를 각 표준편차의 척도 변환 후 유클리드 거리를 계산한 거리이다. 표준화를 하게 되면 척도(scale)의 차이, 분산의 차이로 인한 왜곡을 피할 수 있다. 통계적 거리(Statistical distance) 라고도 한다.

* 마할라노비스

$$
d_{ijj} = (x_{i} - x_{j})' s^{-1} (x_{i} - x_{j})
$$

변수의 표준화와 동시에 변수 간 상관성을 동시에 고려한 통계적 거리이다.

> * R에서 계층적 군집을 수행할 때 병합적 방법을 사용하는 함수에는 hclust, cluster 패키지의 `agnes()`, `mclust()` 함수가 있다. 
> * 분할적 방법을 사용하는 함수에는 cluster 패키지의 `diana()`, `mona()` 함수가 있다.

### 3) 계층적 군집의 특징

매 단계에서 지역적 최적화를 수행하므로, 결과가 반드시 전역적인 최적해라고 볼 수 없다.

병합적 방법에서 한 번 군집이 형성되면 다른 군집으로 이동할 수 없다.



> * 캔버라 거리 : 가충치 있는 맨해튼 거리
> * 체비셰프 거리 : 변숫값의 최대 차이의 절댓값 $D(x, y) = max_i \vert x_i = y_i \vert$
> * 코사인 유사도 : 두 벡터의 내적을 각 벡터의 크기로 나눈 값을 1에서 뺀 것
> * 자카드 계수 : 명목형 데이터에 대한 유사성 척도 (교집합 원소수 / 합집합 원소수)
> * 단순일치계수(Simple Matching Coefficient) = 단순 매칭 계수
> * 순위상관계수(Rank Correlation Coefficient)







## 2 비계층적 군집

### 1) k-평균 군집(k-means clustering)

원하는 군집 수만큼 초깃값 지정하고, 군집을 형성한 뒤, 군집의 평균을 재계산해서 초깃값을 갱신한다. 최종적으로 k개의 군집을 형성한다.

> 군집화가 잘되었는지 확인하는 척도
>
> 1) Dumm Index = 군집 간 거리 중 최솟값 / 군집 내 데이터들 거리 중 최댓값 : 분모 작을수록, 분자 클수록 Dumm Index 커지고 군집화가 잘 되었다고 볼 수 있다.
>
> 2) 실루엣(Silhouette) 지표 : 1인 경우 한 군집의 모든 개체가 모두 붙어있는 경우이다. 0.5보다 크면 군집 결과가 타당하다고 평가하고, 1에 가까울수록 군집화가 잘되었다고 본다.



## 3 혼합분포군집

모형기반(model-based)의 군집 방법이다.  데이터가 k개의 모수적 모형의 가중합으로 표현되는 모집단 모형으로 나왔다는 가정하에 **모수와 함께 가중치를 추정하는 방법**을 사용한다.

**EM 알고리즘**

1) K-means clustering 처럼 랜덤하게 초기화

**2) E-step : 각 자료가 어느 집단에 속하는지에 대한 정보를 가지는 변수(잠재변수, latent variable) 생성한다.**

**3) M-step : 최대우도추정으로 모수를 추정하고, 모수의 값이 수렴할 때까지 반복한다.**



```R
install.packages("mclust")
library(mclust)
mc <- Mclust(iris[, 1:4], G=3)
summary(mc, parameters=TRUE)
```





## 4 SOM(Self-Organizing Maps, 자기조직화지도)

### 1) SOM process

* 인공신경망의 한 종류이다.
* 차원축소, 군집화를 동시에 수행하는 방법이다.
* 입력 벡터를 훈련집합에서 매치되도록 가중치를 조정하는 뉴런 격자에 기초한 자율학습(Unsupervised Learning)의 한 방법이다.

> SOM 기능
>
> * 1) 구조탐색 : 데이터 특징을 파악하여 유사 데이터를 군집화한다.
> * 2) 차원축소, 시각화 : 통상 2차원 그리드에 매핑하여 인간이 시각적으로 인식할 수 있게 돕는다.

* 입력변수의 위치 관계를 그대로 보존하므로, 실제 데이터가 유사하면 지도상에 가깝게 표시된다.



### 2) SOM vs 신경망 모형



```R
install.packages("kohonen")
library(kohonen)
data("wines")
head(wines)
set.seed(7)

wine.som <- som(scale(wines), grid=somgrid(5, 4, "hexagonal")) # scale() 표준화함수
summary(wine.som)
plot(whine.som, main="wine data")

par(mfrow=c(1, 3))
plot(wine.som, type="counts")
plot(wine.som, type="quality")
plot(wine.som, type="mapping")
```





### 



## 5 밀도기반군집(Density-Based Clustering)

임의의 형태의 군집을 찾는데 장점이 있다.



**DBSCAN(Density-based Spatial Clutering of Application with Noise)**

1) eps, minnPts 설정 (2개 파라미터 필요)

2) 잡음점을 군집에서 제외

3) eps 반경 안에 있는 코어점들을 서로 연결

4) 연결된 코어점들을 하나의 군집으로 형성

5) 경계점은 관련된 코어점을 포함하는 군집 중 하나에 할당



```R
install.packages("fpc")
library(fpc)
data(iris)
iris1 <- iris[-5]

df <- dbscan(iris1, eps=0.42, MinPts=5)
table(df$cluster, iris$Species)
plot(df, iris1)
plotcluster(iris1, df$cluster)
```







---

# 04 연관분석

## 1 연관규칙

### 1) 연관규칙의 개념

### 2) 연관규칙의 측정 지표

* 지지도
* 신뢰도
* 향상도

### 3) 연관분석 절차



---

## 05 순차패턴 분석

