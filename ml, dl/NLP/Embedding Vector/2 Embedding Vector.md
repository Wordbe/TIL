#  2 Embedding Vector

## 2.1 자연어의 계산과 이해

> 사람 말을 100% 이해하는 인공지능이 있더라도, 그 이해의 본질은 ‘계산’이다.

자연어의 통계적 패턴(statistical pattern) 정보를 통째로 임베딩에 넣는다. 

### 임베딩을 만드는 철학

|             | Bag of Words 가정                                       | 언어 모델        | 분포 가정(Distributed Hypothesis)                       |
| ----------- | ------------------------------------------------------- | ---------------- | ------------------------------------------------------- |
| 내용        | 문장에 쓰인 단어의 빈도                                 | 단어의 등장 순서 | 단어들이 같이 나타나는 정도                             |
| 대표 통계량 | TF-IDF<br />(Term Frequency-Inverse Document Frequency) | -                | PMI<br />(Pointwise Mutual Information, 점별상호정보량) |
| 대표 모델   | Deep Averaging Network                                  | ELMo, BERT       | Word2Vec                                                |



백오브워드 가정, 언어 모델, 분포 가정은 말뭉치의 통계적 패턴을 서로 다른 각도에서 분석하는 것이고, 상호 보완적이다.



## 2.2 어떤 단어가 많이 쓰였는가

### 2.2.1 백오브워드 가정

**수학에서 백(bag)은 중복 원소를 허용한 집합(multiset)을 말한다. 원소의 순서는 고려하지 않는다.**

단어들을 바구니(bag)에 넣고 흔들어 빈도를 세는 것으로 생각하면 이해하기 쉽다.

정보 검색(Information Retrieval) 분야에서 많이 쓰인다. 질의(query)에 가장 적절한 문서를 보여줄 때, 질의를 BoW 임베딩으로 변환하고 질의와 검색 대상 문서 임베딩 간 코사인 유사도를 구해 유사도가 가장 높은 것을 사용자에게 노출한다.



### 2.2.2 TF-IDF

특정 단어의 빈도가 높더라도 문서의 주제를 가늠하기 어려운 경우가 있다. 예를 들면 ‘은/는’ 등의 조사는 빈도가 많지만 해당 문서의 주제를 추측하기는 어려운 것이 있다.

아래 식과 같이 가중치를 부여해서 행렬 원소를 바꾼다.
$$
\text{TF-IDF}(w) = \text{TF}(w) \times log(\frac{N}{\text{DF}(w)})
$$

* TF(Term Frequency) : 한 문서에서 단어 빈도 수
* $N$  : 전체 문서의 수
* DF(Document Frequency) : 특정 단어가 나타난 문서의 수
* IDF(Inverse DF) : $log(\frac{N}{\text{DF}(w)})$, 값이 클 수록 특이한 단어라는 의미이다.



### 2.2.3 Deep Averaging Network

Iyyer et al., 2015 는 BoW의 뉴럴넷버전이다.

문장의 임베딩은 중복집합(bag)에 있는 단어의 임베딩을 평균을 취해 만든다. 이러한 문장 임베딩을 입력받아 문서의 범주를 분류한다. 간단한 구조이지만 성능이 좋아 현업에서도 자주 쓰인다.



---

## 2.3 단어가 어떤 순서로 쓰였는가

### 2.3.1 통계 기반 언어 모델

**언어 모델(Language Model)이란 단어 시퀀스에 확률을 부여하는 모델이다.**

단어가 n개 주어진 상황이라면 n개 단어가 동시에 나타날 확률, 즉 $P(w_, \dots, w_n)$을 반환한다. 말뭉치에서 단어 시퀀스의 빈도를 세어 학습한다.

n-gram이란 n개의 단어를 뜻하는 용어이다. 또는 n-gram에 기반한 언어 모델을 의미하기도 한다. 말뭉치 내 단어들을 n개씩 묶어 그 빈도를 학습했다는 뜻이다.

어떤 표현(문장 일부) 다음에 특정 단어가 나타날 확률을 **조건부확률**의 정의를 활용해서 최대 우도 측정법(MLE)으로 유도하면 아래 식과 같다.
$$
P(b|A-b) = \frac{Freq(A)}{Freq(A-b)}
$$
그런데 $Freq(A) = 0$ 이면 문제가 발생한다. 따라서 n-gram 모델을 사용하여, 직전 n-1개 단어의 등장 확률로 전체 단어 시퀀스 등장 확률을 근사한다. 한 상태의 확률은 그 직전 상태에만 의존한다는 마코프 가정(Markov Assumption)에 기반한 것이다.

특별히 bigram (2–gram)은 아래와 같은 식을 따른다.
$$
\begin{gather}
P(w_n \vert w_{n-1}) = \frac{P(w_{n-1} \cap w_n)}{P(w_{n-1})} \newline
P(w_1^n) = P(w_1, w_2, \dots, w_n) = \prod_{k=1}^n P(w_k \vert w_{k-1})
\end{gather}
$$




* 백오프(back-off) : n-gram 등장 빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사 → n보다 작은 gram은 0이 아닐 확률이 더 작기 때문이다.
* 스무딩(smoothing) : 등장 빈도 표에 모두 k만큼 더하는 기법, Add-k smoothing이라 불리기도 한다. k=1인 경우 라플라스 스무딩이라 한다.



### 2.3.2 뉴럴넷 기반 언어모델

발 없는 말이 (언어 모델) 천리

마스크 언어 모델은 일방향이 아닌, 양방향 모델이고, 기존 언어 모델 기법 대비 임베딩 품질이 좋다. (예. BERT)



---

## 2.4 어떤 단어가 같이 쓰였는가

### 2.4.1 분포 가정

NLP에서 분포란 특정 범위, 즉 윈도우(window) 내 동시에 등장하는 이웃 단어 또는 문맥(context)의 집합이다.

분포가정은 “단어의 의미는 곧 그 언어에서의 활용이다.”라는 언어학자 비트켄슈타인 철학에 기반한다.



### 2.4.2 분포와 의미 (1) 형태소

형태소(morpheme)란 어휘적 또는 문법적 의미를 가진 최소 단위를 말한다.

계열관계(paradigmatic relation) : 형태소 자리에 다른 형태소가 대치되어 쓰일 수 있는가를 판별한다.



### 2.4.3 분포와 의미(2) 품사

품사란 단어를 문법적 성질의 공통성에 따라 분류한 것이다. 품사 분류 기준은 기능(function), 의미(meaning), 형식(form)이 있다.

기능이 중요하고, 기능은 특정 단어의 문장 내 역할, 분포는 그 단어의 자리는 어디인지 나타낸다.

### 2.4.4 점별 상호 정보량 (PMI)

PMI는 두 확률변수(random variable) 사이 상관성을 계량하는 단위이다. 두 확률변수가 독립이면 값이 0이 된다. 두 단어의 등장이 독립일 때 대비해 얼마나 자주 같이 등장하는지를 수치화 한 것이다.
$$
\text{PMI}(A, B) = log(\frac{P(A, B)}{P(A) \times P(B)})
$$


window가 2라면 타깃 단어 앞 뒤로 2개의 문맥 단어의 빈도를 계산한다.



### 2.4.5 Word2Vec

Google 2013 발표

CBOW 모델은 문맥 단어들로 타깃 단어 하나를 맞추는 과정에서 학습된다. Skip-gram 모델은 타깃 단어를 가지고 문맥 단어가 무엇일지 예측하는 과정에서 학습된다. 둘 모두 특정 타깃 단어 주변의 문맥, 즉 분포 정보를 임베딩에 함축한다.