# Cleaning and Normalization

> **정제, 정규화**

코퍼스에서 용도에 맞게 토큰을 분류하는 것을 토큰화(tokenization)라고 합니다.

토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning), 정규화(normalization)하는 일이 항상 함께있습니다.

* 정제 : 코퍼스로부터 노이즈 데이터 제거
* 정규화 : 표현 방법이 다른 단어들을 통합시켜 같은 단어로 만듦



---

## 1) 규칙에 기반한 표기가 다른 단어들의 통합

USA와 US는 같은 의미를 지니므로, 하나의 단어로 정규화할 수 있습니다.

uh-huh와 uhhuh는 형태는 다르지만 같은 의미를 갖고 있습니다. 정규화를 거치게  되면, US를 찾아도 USA를 함께 찾을 수 있게 됩니다.

표기가 다른 단어를 통합하는 방법은 어간 추출(stemming)과 표제어 추출(lemmatization)에서 배워 봅니다.



---

## 2) 대, 소문자 통합

영어권 언어에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 정규화 방법입니다. 영어권 영어에서 대문자는 문장의 맨 앞 등과 같은 특정 상황에서만 쓰이고, 대부분 글은 소문자로 작성되기 때문에, 대문자를 소문자로 변환하는 작업은 간단합니다.

<br>

물론, 대문자와 소문자를 무작정 통합해서는 안 됩니다. 구분되어야 하는 경우가 있기 때문입니다. 가령 US(미국)과 us(우리)는 구분이 되어야 합니다. 또한 회사 이름(General Motors)나, 사람 이름(Bush) 등은 대문자로 유지되어야 합니다.

<br>

따라서 일부만 소문자로 변환하는 방법이 있습니다. 문장의 맨 앞에 나오는 단어의 대문자만 소문자로 바꾸고, 다른 단어들은 전부 대문자인 상태로 놔두는 것입니다.

<br>

이러한 작업은 머신 러닝 시퀀스 모델로 더 정확하게 진행할 수 있습니다. 더 많은 변수를 사용해서 소문자 변환을 언제 사용할 지 결정하는 것입니다.

<br>

하지만, 예외 사항을 크게 고려하지 않고, 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 되기도 합니다.



---

## 3) 불필요한 단어의 제거

정제 작업에서 제거해야하는 노이즈 데이터(noise data)는 다음과 같습니다.

* 자연어가 아니면서 의미를 갖지 않는 글자 : 특수 문자 등
* 분석하고자 하는 목적에 맞지 않는 불필요한 단어

<br>

**불필요한 단어를 제거하는 방법**

* 불용어 제거
* 등장 빈도 적은 단어 제거
* 길이가 짧은 단어 제거

<br>

**(1) 등장 빈도가 적은 단어(Rare words)**

너무 적게 등장해서 자연어 처리에 도움이 되지 않는 단어를 제거합니다.

**(2) 길이가 짧은 단어(Words with very short length)**

영어권 언어에서는 길이가 짧은 단어를 삭제하는 것만으로도 어느정도 의미가 없는 단어들을 제거하는데 효과를 볼 수 있다고 알려져 있습니다.

즉, 영어권 언어에서 길이가 짧은 단어는 대부분 불용어에 해당합니다.

길이가 짧은 단어를 제거하는 두 번째 이유는 길이를 조건으로 텍스트를 삭제하면서 단어가 아닌 구두점들까지도 한꺼번에 제거하기 위함입니다.



하지만, 한국어에서는 이 방법이 크게 유효하지 않을 수 있습니다.

* 확실하진 않지만, 영어 단어의 평균길이는 6~7, 한국어 단어는 2~3 정도로 추정됩니다.
* 예) 학교와 school, 용과 dragon



영어는 길이가 2~3이하인 단어를 제거하는 것만으로도 크게 의미를 갖지 못하는 단어를 줄일 수 있는 효과를 가집니다.

1) 길이가 1인 단어를 제거하면 의미를 갖지 못하는 단어 관사 a와 주어 I 등이 제거됩니다.

2) 길이가 2인 단어를 제거하면 it, at, to, on, in, by 등과 같은 대부분 불용어가 제거 됩니다.

3) 길이가 3인단어도 제거할 수 있지만, fox, dog, car 같은 명사들이 제거되기 시작하므로 고민이 필요합니다.



길이가 1~2인 단어를 정규표현식을 이용하여 삭제

```python
import re
text = "I was wondering if anyone out there could enlighten me on this car."
shortword = re.compile(r'\W*\b\w{1,2}\b')
print(shortword.sub('', text))
>>
was wondering anyone out there could enlighten this car.
```



---

## 4) 정규표현식(Regular Expression)

얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해 이를 제거할 수 있는 경우가 많습니다.

* HTML문서 : HTML 태그
* 뉴스 기사 크롤링 : 게재 시간

또한 전처리를 위해 정규 표현식이 자주 사용됩니다.





---

Reference

https://wikidocs.net/21693