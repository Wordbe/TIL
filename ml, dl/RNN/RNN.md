# 1. 순차데이터(Sequantial data)

데이터에는 시간성(Temporal property)을 가진 데이터가 많다.

심전도(ECG) 신호, 주식 시세, 음성 신호, 문장, 유전자 열 등등이 있다.

이런 데이터는 특징에 순서가 있다는 의미에서 순차데이터라고 한다.

**시간성 정보**를 잘 이용해야 데이터의 특징을 잘 다룰 수 있다.



MLP 에 입력하는 데이터는 정적인 반면, 시간성 데이터는 시간에 따라 내용이 변하므로 동적이고, 길이가 가변적이다. 매우 긴 패턴을 처리하는 연구가 활발히 진행되고 있다.

예를 들어, 긴 문장은 멀리 떨어진 두 단어 사이의 문맥을 이해해야 하는데 이를 장기 의존성(long-term context dependency)이라 한다. 현재는 RNN에 선별 기억 기능을 추가한 LSTM이 널리 이용된다.

## 1-1) 순차데이터의 표현

**텍스트 순차 데이터 표현**

>  예) "April is the cruelest month"



**방법 1** Bow(bag of words, 단어가방)

​	사전 단어 각각의 문장 출현 빈도를 세어 보는 방법

​	사전 단어 수 m이 30000 개라고 하면, 위 문장에서 3만개 요소 중 5개 단어만 1이고 나머지는 모두 0이다. 편의상 모두 더하면 1이 되도록 정규화한다.

따라서,
$$
x = (0.0, \cdots , 0.2, \cdots, 0.2, \cdots)^T
$$
처럼 표현하고, 벡터 x의 차원은 사전의 크기 m이 된다.

하지만, RNN에는 적절하지 않다. "The cruelest month is April" 과 같은 방식으로 표현되므로, 시간성이 모두 사라지기 때문이다.



**방법 2** One-hot code(원핫코드)

만약 사전에 April이 세번째, is가 다섯 번째라면, 그 자리만 1, 나머지는 0인 원핫벡터를 만든다.
$$
x = ((0, 0, 1, 0, 0, \cdots)^T, (0, 0, 0, 0, 1, 0, \cdots)^T, \cdots)^T
$$
하지만 한 단어를 표현하는데 m개의 숫자를 사용해야 하고, 서로 다른 단어 사이의 유사도를 측정하는 기능도 없다.



**방법 3** word embedding(단어 임베딩)

​	단어 사이의 상호작용을 분석함으로써 새로운 공간으로 변환한다. (word2vec)

보통 사전의 크기 m보다 훨씬 낮은 차원으로 변환한다. 한 예로는 m=30,000 인 사전을 620 차원으로 줄였다. (Cho, 2014)

Tensorflow의 단어임베딩 함수로, t-SNE 분석을 통해 데이터를 시각적으로 볼 수도 있다.

의미적으로 유사한 단어가 임베딩 공간에 가까이 분포되어 있다.



## 1-2) 순차 데이터 특성

* 샘플마다 길이가 다르다. →  hidden layer에 순환에지(recurrent edge)를 부여함으로써 가변길이를 수용한다.
* 문맥 의존적(context dependent)이다. 비순차 데이터가 공분산으로 특징이 규정된 반면, 순차 데이터는 두 특징 사이에 고정된 관련성이 없고, 대신 하나의 샘플 안에서 두요소 $x^{i}$ 와 $x^j$ 사이에 문맥 의존성이 있다.

---











# 2. 순환신경망

sequantial data를 처리하는 신경망은 세가지 기능을 갖추어야 한다.

> 1 시간성: 특징을 순서대로 한 번에 하나씩 입력
>
> 2 가변 길이: 길이가 T인 샘플을 처리하기 위해, 은닉층이 T번 나타나야 한다.
>
> 3 문맥 의존성: 이전 특징 내용을 기억하여 적절한 순간에 활용해야 한다.



MLP 구조를 조금 확장한다.

## 2-1) 구조

![](https://i.ibb.co/ZSNSmXJ/image.png)



![](https://i.ibb.co/8XvJtfH/image.png)



RNN은 은닉층이 하나이다.

그림을 보면 은닉층 사이에 순환에지(recurrent edge)가 있어서, 시간성, 가변 길이, 문맥 의존성이라는 세가지 기능을 갖출 수 있다. 

recurrent edge는 t-1순간에 발생한 정보를 t 순간으로 전달하는 역할을 한다.

(b)를 (c)와 같이 펼치면,

t순간의 $h^t$ 는 입력값 $x^t$ 과 직전 순간의 은닉층값 $h^{(t-1)}$ 에 따라 결정된다는 사실을 보여준다.

은닉층값은 신경망의 '상태'를 뜻하므로, t순간의 상태는 직전 순간의 상태에 연관을 받는다는 것과 잘 들어 맞는다.
$$
h^{(t)} = f(h^{(t-1)}, x{(t)}; \theta)
$$


MLP의 은닉층이 단순 중간계산 결과가 거쳐 가는 임시변수에 불과하다면,

RNN의 은닉층은 이전 정보를 '기억'하는 역할을 한다. 

고로 RNN의 은닉층 노드를 '상태' 변수라고 한다.





RNN 구조를 살펴보면, 매개변수, 즉 가중치를 공유한다는 것을 알 수 있다.

가중치 공유 장점

* 추정할 매개변수의 수를 획기적으로 줄여 최적화 문제를 합리적 크기로 유지한다.
* 매개변수가 특징 벡터 길이 T와 무관하게 일정하다.
* 특징이 나타나는 순간 뒤바뀌더라도 같거나 유사한 출력을 만든다.



그런데 은닉층의 노드 개수는 한정되어있고, 기억력에 한계가 있다.

즉 모든 순간의 입력을 같은 비중으로 기억하므로, $x^{(t)}$에 대한 기억은 갈수록 희미해질 수 밖에 없다.



그림 8-4(a)를 보면 매개변수는 다음과 같다.

* 은닉층으로 가는 에지 가중치 U
* 은닉층과 은닉층 연결하는 순환 에지의 가중치 W
* 은닉과 출력층 연결하는 에지 가중치 V
* 항상 값이 1인 바이어스 노드와 연결된 가중치 b, c

$$
\theta = \{U, W, V, b, a\}
$$



## 2-2) 동작

MLP와 상당히 유사하고, 은닉층사이의 서로 연결된 노드와 가중치만 식에 추가되어 있다.

은닉층의 활성함수로는 tanh가 많이 쓰인다.

출력층에서는 마지막 클래스를 뽑아내기 위해 주로 softmax를 사용한다.



**현재 시간의 은닉층 = 이전 시간의 은닉층 + 현재 시간의 입력층 + bias**
$$
h^{(t)} = \tau(a^{(t)})
$$

$$
where, a^{(t)} = Wh{(t-1)} + Ux^{(t)} + b
$$

**출력층**
$$
o^{(t)} = Vh^{(t)} + c
$$

$$
y'^{(t)} = softmax(o^{(t)})
$$

y'는 예측값



$x^{(1)}$이 바뀌면 h, y'가 영향을 받으므로, RNN의 은닉층은 $x^{(1)}$을 기억한다고 할 수 있다.

또는 $x^{(1)}$은 이후 입력인 $x^{(2)}$, $x^{(3)}$, $x^{(4)}$ 와 상호작용을 한다고 해석할 수 있다.

이 상호작용은 문맥의존성을 제공한다.



## 2-3) BPTT 학습

RNN에서는 SGD를 수행하는데, backpropagation이 필요하다.

하지만 RNN은 시간성 정보를 가지므로 DMLP에서 사용했던 오류 역전파 알고리즘을 그대로 적용할 수 없다. 

대신, 오류 역전파를 개조한 **시간 역전파 알고리즘**(BPTT, back-propagation through time)을 이용한다.(Pearlmutter, 1995)





BPTT에 앞서 잠시 RNN과 DMLP의 차이점을 짚고 넘어가보자.

![](https://i.ibb.co/9pjkTKQ/image.png)

* RNN은 매 순간 입력과 출력이 있다. DMLP는 입력층, 출력층이 하나씩 있다.
* RNN은 가중치를 공유 한다.
* RNN은 공유하는 가중치가 하나이므로 W로 표기하면 된다.



---

**목적함수 정의**
$$
\widehat{\theta} = \underset{\theta}{\mathrm{argmin}} J(\theta) = \underset{\theta}{\mathrm{argmin}} \sum_{t=1}^T J^{(t)} (\theta)
$$
레이블과 예측값을 비교하여 MSE나 Crossentropy , 또는 log loss를 주로 사용한다.



---

**그레디언트 계산**

![](https://i.ibb.co/fkwZRH5/image.png)









# 2-4) 양방향 RNN

문자 모양이 같은 경우 앞뒤에 다른 단어와 상호작용하면서 문맥 정보를 정하게 만들 수 있다.

양방향그래프(Bidrectional RNN)는 은닉층이 두개고 기존 왼쪽에서 오른쪽으로 흐르는 노드에 반대방향으로 정보가 흐르도록하는 연결을 한다.

![](https://i.ibb.co/WnpdBHy/image.png)







---







# 3. 장기 문맥 의존성

챗봇이 상대방에게 똑똑하게 응대하려면 문맥 의존성을 제대로 이해해야 한다.

주어와 서술어가 멀리 떨어지는 등, 관련된 요소가 멀리 떨어져 있는 경우를 장기 의존성(long-term dependency)라 한다.



RNN에서는 그레이언트 소멸(gradient vanshing)문제 또는 그레디언트 폭발(gradient explode)가 MLP보다 심각하다.

RNN은 입력 샘플 길이에 따라 은닉층 계산이 반복되는데, 음성인식, 문장분석 등 많은 응용영역에서는 매우 긴 샘플들이 많기 때문이다.

또한 RNN은 가중치를 공유하기 때문에, W를 반복적으로 적용하므로 계속 작아지거나 커져 문제가 훨씬 심각하다.









Reference

* 기계학습 - 순환 신경망, 오일석, 2017