# 4. LSTM

**LSTM(Long short term memory)**(Hochretier, 1997)은 여러 종류의 게이트가 있어 입력을 선별적으로 허용하고, 계산 결과를 선별적으로 출력할 수 있다.

**GRU(Gated recurrent unit)**은 성능 저하를 최소로 유지하면서 LSTM을 단순화한 모델이다.(Chung, 2014)



## 4.1) 게이트 이용한 영향력 범위 확장

LSTM의 핵심 아이디어는 게이트를 열고 닫는 것이다.

게이트는 0.0~1.0사이의 실숫값을 가지고 개폐 정도를 조절한다. 언제 얼마만큼 여닫을지는 학습으로 알아낸다.



구조는 RNN의 은닉층에 메모리블록을 배치한 것이다.이 메모리 블록에는 입력 게이트와 출력 게이트가 있다.

![](https://i.ibb.co/wMCK175/image.png)



## 4.2) LSTM의 동작

![](https://i.ibb.co/vkmx5Tx/image.png)



시간 t에서 j번째 메모리 블록을 보자.



![](https://i.ibb.co/k3b6gZn/image.png)





메모리로 들어오는 $h^{(t-1)}$입력은 세 방향(입력단, 입력게이트, 출력게이트)으로 들어가게된다. 이를 각각 $W^g, W^i, W^o$ 라고 한다. 물론 이 행렬 중에 한 열이 될 것이다. 

또한 입력 벡터 $x^(t)$ 도 세 군데로 입력이 된다. 이를 각각 $U^g, U^i, U^o$ 라고 한다.

그 후 세 곳에서 이루어지는 연산은 다음과 같다.
$$
\begin{gather}
입력단: g= \tau_g(U^gx^{(t)} + W^gh^{(t-1)} + b^g) \newline
입력 게이트: i= \tau_f(U^ix^{(t)} + W^ih^{(t-1)} + b^i) \newline
출력 게이트: o= \tau_f(U^ox^{(t)} + W^oh^{(t-1)} + b^o)
\end{gather}
$$


$\tau_g$ 는 입력단 활성함수로 [-1, 1] 가 치역인 tanh 를 주로 사용하고,

$\tau_f$는 입력 게이트와 출력 게이트의 활성함수로 [0, 1] 가 치역인 logistic sigmoid 를 주로 사용한다.

다시 그림을 보면,

입력단(g) * 입력게이트(i) 의 값이 0에 가까우면, 입력단을 차단하고,

1에 가깝다면 g를 그대로 위로 올리게 된다.



그 위는 메모리 블록이 '기억'하는 내용이 담기는 부분인데, 아래 식과 같이 업데이트 된다.
$$
s^{(t)} = s^{(t-1)} + g \odot i
$$
위로 올린 값이 0이면 예전의 기억을 그대로 유지하고, 그렇지 않다면 바뀐값이 반영되어 업데이트 되게 된다. 결국 이전 입력의 영향력 범위를 더 멀리 확장하는 효과를 제공한다.

원 안의 기호 /는 선형 활성함수를 의미한다.



이제 위 쪽에 있는 *를 보면, 
$$
h_j^{(t)} = \tau_h(s^{(t)}) \odot o
$$
의 식으로 출력이 된다. 이 때, $ \tau_h $는 활성함수로 주로 tanh를 사용한다.

이 계산 결과는 q개의 출력 노드로 전달되어, 출력단의 계산에 사용된다.

또한, 입력단, 입력게이트, 출력 게이트에 있는 3p개의 노드로 전달되어 다음 순간 t +1계산에 활용된다.



마지막으로 출력단의 계산식이다. RNN과 똑같다.
$$
y'^{(t)} = softmax(Vh^{(t)} + c)
$$




## 4.3) 망각 게이트와 핍홀

LSTM은 여러 측면에서 성능 개선이 이루어졌고,

forget gate(망각 게이트)는 현재 거의 표준처럼 사용된다.(Gers, 2000)

그리고, 또 다른 확장으로 peephole(핍홀) 기능이 있다.(Gers, 2000)

핍홀은 메모리 블록의 내부 상태를 3개의 게이트에 알려주는 역할을 한다.

순차 데이터를 처리하다가, 어떤 조건에 따라 특별한 조치를 취해야 할때, 핍홀이 매우 효과적이다.

![](https://i.ibb.co/QmrdDrn/image.png)

![](https://i.ibb.co/qmhSHNt/image.png)

![](https://i.ibb.co/tMBZ6G9/image.png)













# 5. 응용사례

순환신경망은 주로 가변 길이의 패턴을 처리하는 데 활용된다(Graves, 2012)

* NLP(Natural language processing, 자연어처리) 는 처리해야 할 문장이 짧은 것부터 아주 긴 것 까지 다양하게 발생하므로, 주로 순환 신경망을 이용한다.
* 음성 인식, 주식 예측 등 응용에서 분류나 회귀문제를 푸는 분별 모델(Discriminative model)로 활용된다.
* 새로운 문장이나 영상을 생성하는 등의 응용에서는 생성 모델(Generative model)로 활용된다.



## 5.1) 언어모델(Language Model)

언어 모델은 문장, 즉 단어 열의 확률분포다. 예를 들어 P(자세히, 보아야, 예쁘다) > P(예쁘다, 보아야, 자세히) 인 이유는 "자세히 보아야 예쁘다." 라는 문장이 발생할 확률이 더 높기 때문이다. 

음성 인식기가 인식 결과로 세 후보를 출력했을 때, 언어 모델이 가장 높은 확률을 부여하는 것을 최종 결과로 선택하면 인식 성능을 높일 수 있다. 

이런 응용은 필기 인식, 기계 번역, 영상 주석 생성 등에도 사용할 수 있다.

언어 모델을 구축하는 방법을 소개한다.

---

**n-그램**

자연어 처리 분야가 기계학습 이전 부터 사용하던 모델이다. 문장을 $ x = (z_1, z_2, \cdots, z_T)^T $ 라고 하면 x가 발생할 확률은 다음과 같이 정의된다.
$$
p(z_1, z_2, \cdots, z_T) = \prod_{t=1}^T P(z_t | z_1, z_2, \cdots, z_{t-1})
$$
n-그램은 확률을 계산할 때 n-1개 단어만 고려한다.

따라서 n-그램 모델식 에서는 다음과 근사식이 가능하다.
$$
p(z_1, z_2, \cdots, z_T) \approx \prod_{t=1}^T P(z_t | z_{t-(n-1)}, \cdots, z_{t-1})
$$


사전의 크기가 m일 때 우항의 경우 경우의수가 $m^n$이다. 자연어의 경우 m은 수만~수십이므로, m = 10,000이라고 하면 3-gram은 $10^{12}$ 개의 확률을 알아야 한다. 그래서 현실적으로 n을 작게 할 수 밖에 없다.

확률 추정은 corpus(말뭉치)를 이용한다. 말뭉치는 분야에 따라 수집합 문헌집합 또는 일반적인 문헌집합이 될 수 있다. 



n-그램은 n-1만큼 떨어진 단어 사이의 상호작용만 반영하므로 장기 문맥 의존성 측면에서 한계가 있다.



---

**전방 신경망 모델**

MLP나 CNN 같은 모델을 사용하면 n-그램에서 발생한 차원의 저주를 피할 수 있다.(Bengio, 2003)

n-그램에서는 사전의 크기가 m일 때 단어 하나를 m차원의 원핫벡터로 표현하는데, 전방 신경망 모델은 이 벡터를 훨씬 낮은 q차원의 특징 공간으로 변환하여 표현한다.

예를 들면, 3만 차원을 500차원으로 줄일 수 있다.



신경망 모델은 의미상으로 비슷한 단어가 q차원 특징 공간에서 가까운 곳에 위치한다는 장점이 있다. 예를 들어 {개, 고양이}, {뛴다, 달린다, 걷는다, 긴다.}, {방에서, 거실에서, 마당에서}등이 특징 공간에서 군집을 형성한다. 말뭉치를 훈련집합으로 사용하여 이들이 군집을 형성할 수 있게 신경망을 학습하면 된다.

---

**RNN 모델**

RNN은 현재까지 본 단어 열을 기반으로 다음 단어를 예측하는 방식으로 학습한다.(Mulder, 2015)

예측은 확률 추정에 따라 이루어지므로, 예측 능력을 갖추었다면 확률분포를 갖추었다는 말과 같다.

비지도 학습에 해당하며, 말뭉치로부터 쉽게 훈련집합을 구축할 수 있다.
$$
x = (<시작>, z_1, z_2, \cdots, z_T)^T, y = (z_1, z_2, \cdots, z_T, <끝>)^T
$$
![](https://i.ibb.co/jGRqYjd/image.png)



$\tilde{x_1} = (자세히, 보아야, 예쁘다)^T$ 입력으로 넣은 값에서 출력이 나오면, 각 요소를 $P(\tilde{x_1})$으로 취하고,  

$\tilde{x_2} = (자세를, 모아야, 예쁘다)^T$ 입력으로 넣은 값에서 출력이 나오면, 각 요소를 $P(\tilde{x_2})$으로 취한다.

그 후 확률이 높은 것을 최종 결과로 택하는 방식으로 학습한다.



---

**생성 모델로 활용**

RNN으로 구현한 언어 모델은 예측 기능이 있으므로 생성 모델로 활용할 수 있다.

예로는 구글 댓글 15,000개를 수집하여 RNN을 학습 후 합성문장을 생성했다.(Karpathy, 2015)

또는 글자 단위로 예측하여, 그럴듯한 문학 작품을 만든 사례도 있다.

8천개의 아이 이름으로 학습 후, 새로운 이름을 생성한 사례도 있다.





## 5.2) 기계 번역(Machine Translation)

기계 번역은 입력과 출력의 길이가 달라 sequence-to-sequence 문제이기 때문에 더 까다롭다.

통계적 기계번역이라는 고전적 접근방법은 문장을 개별 단어로 분리하고, 단어를 번역하고, 그 것들을 다시 조립하는 단계를 거친다(Koehn, 2010)

개별적으로 최적화 되어 있는데, 자연어는 변화가 심하고 모호함이 많아 통계적 접근방법이 한계가 있다.



신경망 기계 번역(Neural Machine Translation)은 주로 LSTM을 사용한다. 번역 과정 전체를 통째로 최적화 한다. 

인코더 LSTM에서 문장 x를 받아 $h_{Ts}$를 만들어 디코더 LSTM에 넘기는 방법이다.(Cho et al., 2014)

즉, 가변 길이의 입력 문장을 고정크기의 특징 벡터로 변환 후, 이로부터 출력 문장을 만드는 방식이다.

이 방식은 입력 문장들의 길이가 아주 길거나 짧은 것들이 섞이면 문제가 된다.

![](https://i.ibb.co/94FX2bs/image.png)





이를 개선하기 위해 모든 순간의 상태변수를 이용하는 방법이 나왔다. (Bahdanau, 2015)

인코더의 계산 결과인 $ h_1, h_2, \cdots, h_{Ts} $ 를 모두 디코더에 넘겨준다.

또한 인코더 LSTM은 양방향 구조를 채용하여 어순이 다른 한영, 영한 번역에 유용한 결과를 낸다.

![](https://i.ibb.co/ZBmQ4jC/image.png)





## 5.3) 영상 주석 생성(Image caption generation)

자연 영상을 해석하고 결과를 문장으로 변환해 출력하는 application이다.

자연 영상에 주석을 생성하려면, 영상을 해석하는 부분(CNN)과 문장을 생성하는 부분(RNN)이 필요하다. 



CNN의 FC layer의 차원은 word embedding 공간의 차원과 같다. 즉, 영상과 단어는 같은 임베딩 공간의 점으로 변환된다.
$$
\tilde{z}_1 = CNN(x)
$$

$$
\tilde{z}_t = E z_t, t = 0, 1, \cdots, T
$$

E는 원핫 코드로 표현된 단어 $z_t$를 단어 임베딩 공간으로 변환하는 행렬이다.



![](https://i.ibb.co/gzrVC7f/image.png)



예측한 결과는 주로 log loss를 이용한다.

매개변수는 {CNN, LSTM, word embedding} 매개변수이다.







---



Attention 개념을 추가한 주석의 품질을 높이는 방법(Xu, 2015),

비디오에서 주석을 자동 생성하는 기법(Venugopalan, 2015),

비슷한 환경에서 연달아 촬영한 사진을 같은 문맥으로 주석을 생성하는 기법(Park, 2015),

영상 내용에 관해 자연어 질의어를 주면 자동으로 응답하는 기법(Xiong, 2016) 등 다양한 연구가 진행 되고 있다.









---

**reference**

기계학습, 순환 신경망, 오일석, 2017