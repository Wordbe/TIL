# 비지도학습

훈련집합(training set)이라는 명시적 정보뿐 아니라,

세상의 일반적인 규칙으로 얻을 수 있는 암시적 정보가 있다.

이러한 정보를 사전 지식(prior knowledge)라 하며, 기계 학습에서는 사전지식을 잘 활용함으로 성능향상을 꾀할 수 있다.



다음은 중요한 두 가지 사전 지식이다.

* Manifold hypothesis(매니폴드 가설) : 데이터 집합은 하나 또는 여러 개의 매니폴드를 구성하며, 모든 샘플은 매니폴드와 가까운 곳에 있다.

* Smoothness hypothesis(매끄러움 가설) : 샘플은 어떤 요인에 의해 변화한다. 이 때 매끄러운 곡면을 따라 위치가 변한다.



Supervised, Unsupervised, Semi-supervised learning 모두 매니폴드 가정과 매끄러움 가정을 기반으로 알고리즘 설계를 한다.

지도학습에서는 목적함수(손실함수, loss function)가 학습 과정을 주도한다. 따라서 이 가정들은 목적함수의 모양과 특성을 규정하는 방식으로 암시적으로 나타난다.

비지도 학습 또는 준지도 학습에서는 이 가정들을 좀 더 명시적으로 사용한다. 예로, 군집화는 가까운 샘플을 찾아 같은 군집에 배정하는 연산을 하는데, 이는 매니폴드 가정을 사용하는 것이다.



비지도 학습의 종류는 많다.

군집화, 확률밀도 추정, 공간 변환, 특징 추출, 차원 축소, 데이터 가시화



여기서는 특정 응용에 무관한 일반 과업으로 군집화, 밀도 추정, 공간 변환을 추려내어 설명한다.

![](https://i.ibb.co/rdd4Tc1/image.png)



### **비지도학습의 일반과업**과 응용

1. 군집화(clustering): 특징 공간에서 가까이 있는 샘플을 모아 같은 그룹으로 묶는 일이다. 몇 개의 군집으로 묶을 것인지는 알려질 수도, 알려지지 않을 수도 있다.

   - 영상 분할(mean sift) (Comaniciu, 2002)
   - 생물정보학 유전자 데이터 분석
   - SNS 실시간 검색어 분석하여 사람들의 관심을 알아냄

   

2. 밀도 추정(density estimation): 데이터로부터 확률분포를 추정하는 일이다. 데이터가 조밀하게 분포하는 곳은 높은 확률을, 희소하게 분포하는 곳은 낮은 확률을 배정한다. 가우시안 함수처럼 몇 개의 매개변수로 표현할 수 있는 확률분포를 사용하는 모수적 추정법(parameteric estimation )과 그렇지 않은 비모수적 추정법(non-parametric estimation)이 있다.

   - 확률밀도 함수와 베이즈 정리를 사용, Classification 가능
   - 생성 모델

   

3. 공간 변환(spatial transformation): 데이터가 정의된 원래 특징 공간을 저차원 공간 또는 고차원 공간으로 변환하는 일이다. 새로운 공간은 주어진 목적을 달성하는 데 더 유리해야 한다.

   - 데이터 가시화, 데이터 압축 : 정보 손실 최소화 하여 저차원으로 변환
   - 표현 학습(Representation learning): 특징 추출 알고리즘의 자동 설계, 차원의 크기는 중요하지 않고 분류나 회귀 문제를 푸는 데 더 유리한 공간 변환을 하는가가 중요

   

비지도 학습도 지도학습과 마찬가지로 최적화 문제로 공식화하여 문제를 푼다.





## 3. 군집화(Clustering)

참고 논문: (Xu, 2005), (Jain, 2010), (Fahad, 2014), 

참고 교과서: (Theodoridis, 2009)



### 1) k-means clustering, k-평균 알고리즘

원리가 단순하고, 성능이 좋아 인기 있다.



![](https://i.ibb.co/1nVTXZt/image.png)

처음에 군집 중심을 초기화 하는 방법은 여러가지이다.

n개 중 임의로 k를 뽑아 초기화 할 수 있고,

데이터 분포에 대한 사전 지식이 있다면, 모범 샘플 k개를 뽑아도 된다.



라인 3~4에서 n개의 샘플 각각을 가장 가까운 군집 중심에 배정한다.

라인 6~7에서 각각의 군집 중심을 자신에게 새로 배정된 샘플의 평균으로 갱신한다.

이전 군집 배정과 현재 군집 배정 결과가 같으면 수렴한 것으로 간주하고,

군집 C에 Z를 대입한다.



---

**최적화 문제로 해석**



이 때 k-평균이 사용하는 목적함수는 아래와 같다.
$$
J(Z, A) = \sum_{i=1}^n\sum_{j=1}^k a_{ij} dist(x_i, z_j)
$$


Z는 군집 중심, A는 샘플의 배정 정보를 나타내는 k x n 행렬이다.

i번째 샘플이 j번째 군집에 배정되었다면 $a_{ij}$는 1이고, 그렇지 않으면 0이다.

dist(x, z)는 유클리디언 거리 등 x와 z 사이의 거리를 측정하는 함수이다.





즉, 목적함수 J를 작아지는 방향으로 해를 갱신하는 최적화 문제를 푸는 알고리즘으로 볼 수 있다.

k-평균 알고리즘은 어떤 초기 군집 중심을 잡든 반드시 수렴한다.



하지만, 초기 군집이 달라지면, 최종 결과가 달라질 수 있다.



---

**다중 시작 k-평균 알고리즘(multi-start k-means clustering)**



서로 다른 초기 군집 중심을 가지고 k-평균을 여러번 수행한 다음, 가장 좋은 품질의 해를 선택한다.

n개 중 k개의 샘플을 뽑는 모든 경우의 수에 대해서 해야 하므로,

nPk 만큼의 시간이 곱해질 것이다.

또는 적절한 초기 군집 후보를 주는 방법도 있을 것이다.



---

**EM 기초**

k-평균은 훈련집합 X를 입력받아 최적의 군집집합 C를 출력한다. C는 행렬 A와 등가 표현으로, 서로 바꾸어 쓸 수 있다.

X와 A 이외의 중간 단계에서 Z라는 임시 변수를 사용한다. Z는 입출력단에서 보이지 않고, 중간에 임시로 사용되다 사라지므로 '감추어져 있다'라는 의미에서 latent variable(은닉 변수)라고 한다.

k-평균은 은닉변수 Z의 추정과, 알고리즘의 해인 A의 추정을 번갈아 가며 수행하는 방식으로 동작한다.



![](https://i.ibb.co/hZ07j7K/image.png)

은닉 변수 추정과, 매개변수 추정을 번갈아 수행하면서 최적의 해를 찾는 과정을 EM 알고리즘(Expectation Maximization algorithm)이라고 한다.

은닉변수 Z를 추정하는 단계를 E단계,

매개변수 A를 추정하는 단계를 M단계라고 한다.



### 2) 친밀도 전파 알고리즘 (affinity propagation)

샘플 간 유사도(similarity)로부터 responsibility 행렬 R과 availability 행렬 A라는 두 종류의 친밀도(affinity) 행렬을 계산하고, 이 친밀도로부터 군집을 찾는 알고리즘이다.(Frey, 2007)





## 4. 밀도 추정(density estimation)

어떤 점 x에서 데이터가 발생할 확률, 즉 확률밀도함수 P(x)를 구하는 문제이다.

![](https://i.ibb.co/HzCBvmx/image.png)

직관적으로 $ P(x=x_1) > p(x=x_2) > p(x=x_3) $ 임을 알 수 있다.



### 1) 커널 밀도 추정(kennel density estimation)

위 그림과 같이 각 차원(축)을 여러 구간으로 나누어 특징 공간을 칸(bin)의 집합으로 분할한 다음,

각 칸에 있는 샘플의 빈도를 센다.
$$
P(x) = \frac{bin(x)}{n}
$$


위 빨간 색 박스 칸 안의 샘플 개수는 4이다.

즉, $ n = 80,  bin(x=x_2)=4$

이므로, $P(x=x_2) = 0.05 $이다.

 

이를 이용하여 확률밀도함수를 추정하는 방법을 **히스토그램 방법(histogram method)**라고 한다.



## 2) 가우시안 혼합(Gaussian mixture)





**최적화 문제**
$$
\widehat{\theta} = \underset{\theta}{argmax} \; logP(\mathbb{X}| \theta)
$$




## 3) EM 알고리즘



![](https://i.ibb.co/nPKnJrG/image.png)

2개의 가우시안이 자신에 속하는 샘플이 어느 것인지 알면,

소속된 샘플을 이용하여 평균과 공분산 행렬을 다시 추정하여 자신을 개선할 수 있다.



이처럼 가우시안이 자신의 매개변수를 개선하면, 개선된 가우시안으로 샘플의 소속 정보를 개선할 수 있다.



샘플 $x_i$(n개)가 j번째 가우시안(k개)에 속할 확률을 $z_{ji}$ 라 하자.

이 확률 정보는 k x n 크기의 행렬 Z에 저장할 수 있다.



![](https://i.ibb.co/NW6xKnx/image.png)

> Input : 훈련집합 $\mathbb{X} = \{x_1, x_2, \cdots, x_n\}$, 가우시안 개수 k
>
> Output: 최적의 가우시안과 혼합 계수 $\theta = \{\pi = (\pi_1, \cdots, \pi_k), (\mu_1, \sum_1), \cdots, (\mu_k, \sum_k)\}$
>
> 1 $\theta$를 초기화
>
> 2 while (!멈춤조건)
>
> 3 	$\theta$를 이용하여 소속확률 행렬 Z를 추정한다 // E-step
>
> 4	Z를 이용하여 $\theta$를 추정한다. // M-step



![](https://i.ibb.co/rbGH5X0/image.png)

(6.14)를 라인 3에, (6.15)를 라인 4에 적용한다.



EM 알고리즘(Dempster, 1977)은 위에 최적화 문제 식처럼, 일종의 MLE(Maximum likelihood estimation)이다.

주로 불완전한 데이터, 즉 손실 정보가 포함된 데이터가 주어진 경우에 적용한다. 가우시안 혼합에서는 샘플의 소속 정보가 손실된 것으로 여기면 된다. 



EM알고리즘은 현재 기계 학습, 패턴 인식, 컴퓨터 비전에 두루 활용 된다.

참고: (Bilmes, 1998), (Moon, 1996)







# 5. 공간 변환의 이해

공간변환은 군집화, 밀도 추정과 더불어 비지도 학습의 중요한 세 과지 과업이다.

딥러닝이 기계 학습의 주류가 되면서 공간 변환의 중요성은 더 커지고 있다.



![](https://i.ibb.co/zNrcyck/image.png)



최종적으로 변환된 공간에서는 1차원에서도 두 군집을 분류할 수 있게 된다.



실제 문제에서는 데이터의 특징 공간이 매우 고차원이고, 데이터를 변환하는 목적도 매우 다양하다. 따라서 비지도 학습이 데이터 구조를 스스로 파악하여 최적의 공간 변환을 알아내는 방법을 사용해야 한다.


$$
\widehat{x} = g(f(x))
$$
원래 특징 공간 x는 인코딩되어 $z = f(x)$ 가 되고,

변환공간 z는 디코딩  $\widehat{x} = g(z)$ 가 되는데, $x$와 $\widehat{x}$가 가급적 같아지도록 만드는 것이 목표이다.



데이터 가시화에서는 디코딩 없이 원래 공간을 2차원 또는 3차원의 z공간으로 인코딩 후 컴퓨터그래픽스 기술을 이용하여 그림으로 표현하면된다.



공간변환은 선형인자모델(Linear factor model), 오토인코더(Autoencoder), 매니폴드학습(Maniford learning)으로 구분한다.



---

reference

기계학습, 오일석, 2017 : 6. 비지도 학습

