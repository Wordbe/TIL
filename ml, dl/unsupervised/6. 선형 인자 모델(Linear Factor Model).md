# 6. 선형 인자 모델(Linear Factor Model)

인자(factor)란 관측되지 않는 변수를 뜻합니다.

![](https://i.ibb.co/WGF3BFr/image.png)

z가 인자에 해당하며, 잠복변수(latent variable) 또는 은닉변수(hidden variable)이라고도 불립니다.

선형인자모델은 선형연산을 통해 관찰한 데이터를 인자로 변환하는 방법입니다.



주어진 훈련집합 x 에서 평균이나, 공분산 등의 통계를 내어 데이터를 요약하고 분석할 수도있지만,

선형인자모델을 활용해 데이터의 잠재적인 특성을 파악하여 더 심층적인 의사결정을 할 수 있습니다.



일반적으로 차원(특징)의 크기는 z < x 이며, 아래와 같이 선형 연산을 사용하여 인코딩, 디코딩을 표현합니다.
$$
f : z = W_{encoder}x + \alpha_{encoder}
$$

$$
g: x = W_{decoder} z + \alpha_{decoder}
$$

인자 z와 추가 항 $\alpha$에 따라 서로 다른 여러가지 모델이 가능합니다.





## 1. 주성분 분석(PCA, Principle Component Analysis)

주성분분석은 데이터를 원점 중심으로 옮겨 놓는 일부터 시작합니다. 다음 식을 이용합니다.
$$
\begin{cases}
	x_i = x_i - \mu, \\
	where, \mu = \frac{1}{n}\sum_{i=1}^n x_i
\end{cases}
$$
$\mu$는 훈련집합에 있는 샘플의 평균입니다.




$$
\begin{cases}
	z = W^T x \\
	where, W = (u_1 u_2\cdots u_q), and \ u_j = (u_{1j}, u_{2j}, \cdots u_{dj})^T
\end{cases}
$$
q개의 벡터 내적, 즉 $u_j^Tx$ 가 수행되어 $d$차원의 x가 q차원의 z로 변환됩니다. 각각의 벡터 내적은 $u_j$가 가리키는 축으로의 투영에 해당합니다. 

![](https://i.ibb.co/7KQW80D/image.png)



위 그림에서는 q = 1이므로 $u_1$대신 간단히 $u$라고 표시합니다. 벡터는 하나의 직선을 이루고, 각 점이 그 직선위로 투영된다고 생각하시면 좋을 것 같습니다.

고차원 데이터가 저차원으로 변환될 때, 정보 손실이 일어납니다. (a), (b)에서는 원래 다른 점이 었던 데이터가 같은 점으로 매핑된 것을 볼 수 있습니다. 이처럼 PCA에 의한 변환은 정보 손실을 일으키는 데, (c)처럼 이를 최소화하는 축을 찾는 것이 문제입니다.



### 학습 알고리즘

PCA는 변환된 새로운 공간에서 샘플이 얼마나 퍼져 있는지를 따져 정보 손실을 측정합니다. 즉, 변환 공간 $\mathbb{Z} =  \{z_1, z_2, \cdots, z_n\}$ 의 분산이 클수록 정보 손실이 작다고 판단합니다.

 

**최적화 문제 1**

$\mathbb{Z} =  \{z_1, z_2, \cdots, z_n\}$ 의 분산을 최대화하는 q개의 축  $W = (u_1, u_2, \cdots, u_q)$ 을 찾아라.



$x$를 원점을 중심으로 분포하도록 재배치 하였으므로, $\bar{z} = 0$이고, $\mathbb{Z}$의 분산을 다음과 같이 구할 수 있습니다. 
$$
\sigma^2 = \frac{1}{n} \sum_{i=1}^n (z_i - \bar{z})^2 = \frac{1}{n} \sum_{i=1}^n z_i^2 = \frac{1}{n} \sum_{i=1}^n (u^T x_i)^2 = u^T \Sigma u
$$
$\Sigma$는 $\mathbb{X}$의 공분산 행렬입니다.



**최적화 문제 2**

분산 $\sigma^2$를 최대로 하는 $u$를 찾아라.



$u$가 길수록 분산이 커지므로, $u$가 단위 벡터라는 조건 $u^T u = 1$을 이용하여 라그랑주함수 $L(u)$를 만들어 문제를 바꾸어 쓸 수 있습니다.



**최적화 문제 3**

$L(u) = u^T \Sigma u + \lambda (1 - u^T u )$를 최대로 하는 $u$를 찾아라.



$L(u)$를 $u$로 미분하고 수식을 전개하면 다음과 같습니다.
$$
\frac{\partial L(u)}{\partial u} = 2\Sigma u - 2\lambda u
$$
$L(u)$를 최대로 하는 해는 $\frac{\partial L(u)}{\partial u} = 0$ 임을 적용하면 다음을 얻습니다.
$$
\Sigma u = \lambda u
$$
위 식을 풀면 $d$개의 고윳값과 고유 벡터를 얻고, 이 벡터들을 서로 수직입니다.(고유벡터)

이들을 각각 주성분(principal component)이라 하며, 주성분의 성질에 따르면 고윳값이 클수록 많은 정보를 유지합니다. 이 성질에 따라 고유값이 큰 순서대로 고유벡터를 나열하여 $u_1, u_2, \cdots, u_d$라고 하면, d차원을 q차원으로 축소하기 위해 q개를 택하여 $W$ 에 넣으면 됩니다.





![](https://i.ibb.co/QKChNZq/image.png)



여기까지 보면 변환에 쓸 행렬 W를 구하였고, 인코딩을 성공했습니다.

디코딩은 $z = W^T x$을 통해 할 수 있는데, W가 정규직교 행렬이므로 $W^T = W^{-1}$를 만족합니다.

따라서,
$$
x = (W^T)^{-1} z = Wz
$$
가 됩니다.

보통 q < d로 설정하여, 인코딩을 통해 차원을 축소하고, 데이터의 특징을 보존하는 저차원으로 공간 변환을 이루었다가 다시 디코딩하는 방식으로 원본과 가까운 데이터를 얻습니다.





## 2. 독립 성분 분석(ICA, Independent Component Analysis)

'블라인드 원음 분리(blind source separation)' : 두 가지 이상 다른 종류의 소리를 분류하는 신호 복원 문제



![](https://i.ibb.co/NNtG15g/image.png)
$$
\begin{gather}
x = Az \\
\tilde{z} = W x, where \ \ W = A^{-1}
\end{gather}
$$
원음을 $z$라 하고 혼합신호를 $x$라 하면, a와 $z$의 선형결합을 통해 $x$를 만들 수 있습니다. 이는 소리의 물리법칙에 따른 것으로 $z_1, z_2$은 서로 독립된 신호일 때 성립 가능합니다.



### 독립성 가정

독립성 가정은 음성, 영상, 뇌파(EEG) 등 다양한 신호가 만족하므로 ICA를 적용할 수 있습니다.

독립성의 수학적 정의는 다음과 같습니다.
$$
P(z) = P(z_1, z_2, \cdots, z_d) = \prod_{j=1}^d P(z_j)
$$

>  ICA는 독립성을 사용합니다. 두 확률변수가 독립(independence)이면 반드시 비상관(uncorrelation)이지만, 역은 성립하지 않습니다. PCA의 경우 비상관 조건을 이용합니다.







### 비가우시안 가정

ICA는 독립성 뿐아니라 비가우시안 가정도 만족해야 합니다. 원래 신호 $z_i$가 가우시안 분포를 따르면, $z_i$의 선형 결합으로 만들어진 $x_i$도 가우시안 분포를 따르며, $x_1, x_2, \cdots, x_d$의 결합분포 P(x)도 가우시안이 되기 때문에 원래 신호로 복원할 정보가 사라진다고 할 수 있습니다.



![](C:\Users\seong\AppData\Roaming\Typora\typora-user-images\1574343098904.png)



반면, 비가우시안 일경우 두 신호를 분리할 가능성이 생깁니다.

ICA는 $\mathbb{z_j}$가 비가우시안인 정도를 최대화하는 $\mathbb{w_j}$를 구합니다. G는 비가우시안 정도를 측정하는 함수입니다.


$$
\widehat{\mathbb{w}_j} = \underset{\mathbb{w}_j}{\mathrm{argmax}} \ G(\mathbb{z}_j)
$$


**ICA 학습**

1) 훈련집합의 평균이 0벡터가 되도록 이동 변환

2) Whitening 변환 전처리 
$$
x'_i = (D^{-\frac{1}{2}} V^T) x_i
$$
​	V는 $\mathbb{X}$의 공분산 행렬의 고유벡터를 열에 비치한 d x d 행렬, D는 고윳값을 대각선에 배치한 d x d 대각행렬입니다.

3) $\mathbb{w_j}$를 임의값으로 초기화 

4) 첨도가 작아지는 방향으로 $\mathbb{z_j}$가 비가우시안인 정도를 최대화하는 $\mathbb{w_j}$ 추정

​	여기서 G로 첨도(kurtosis)를 주로 사용합니다. 첨도는 4차 모멘트까지 고려한 값으로 다음과 같이 정의합니다.

![](https://i.ibb.co/MDh7Sy4/image.png)

​	검은색 그래프는 가우시안이며, 가우시안의 첨도는 0입니다. 분포가 뾰족해질수록 첨도가 커지고, 펑퍼짐해지면 음수가 됩니다.



**PCA와 ICA 비교**

| PCA                           | ICA                                               |
| ----------------------------- | ------------------------------------------------- |
| 가우시안과 비상관 전제        | 비가우시안과 독립성 전제                          |
| 2차 모멘트까지 사용           | 4차 모멘트까지 사용                               |
| 인코딩 후 찾은 축은 서로 수직 | 대부분 수직이 아님                                |
| 차원 축소 용도                | 밀도 추정, 특징 추출, 생성 모델, 대부분 신호 분리 |





## 3. 희소 코딩(Sparse coding)



![](https://i.ibb.co/VghsqJ6/image.png)



자연에서 발생하는 각종 신호는 기저함수, 기저 벡터의 선형결합으로 표현할 수 있습니다. 푸리에변환, 웨이블릿 변환이 대표적인 기법입니다.



조각 영상을 여러 개의 기저 벡터 선형 결합으로 표현하는 것을 희소코딩이라 합니다.

조각영상을 x, i번째 기저 벡터를 $d_i$, 계수집합을 a로 표기하면, 희소코딩 과정은 다음과 같습니다.
$$
\begin{cases}
	x = Da \\
	where, D = (d_1 d_2 \cdots d_m)
\end{cases}
$$
d는 조각 영상의 화소 개수이자 벡터 x의 차원 수입니다. 희소 코딩에서는 D를 사전(dictionary), $d_i$를 사전 요소, 벡터 a는 희소코드(sparse code)라고 합니다.



희소 코딩에서는 기저함수를 사람이 설계하는 것이 아닌, 비지도 학습이 훈련집합 X를 통해 알아냅니다. 또한 사전의 크기를 과잉완벽(overcomplete)하게 알아내며, 표현된 대부분의 벡터는 값이 0이고 일부만 0이 아닙니다. 여기에서 '희소'라는 명칭이 붙었습니다. 



희소코딩은 최적의 사전과 최적의 희소코드를 알아내는 것이 목표입니다.
$$
\widehat{D}, \widehat{A} = \underset{D, A}{argmin} \sum_{i=1}^n \Vert x_i - Da_i\Vert^2_2 + \lambda \phi(a_i)
$$
이를 푸는 방법은 D와 A를 번갈아가며 계산하는 방법(Lee, 2006),

k-SVD(Aharon, 2006) 등이 있습니다.



# 마무리

선형 인자 모델에 속하는 PCA, ICA, sparse coding을 배웠습니다.

모두 선형 연산을 사용하여 인자를 추정하는 공통점이 있습니다.

각자 고유의 쓰임 영역이 있고, 오늘날 까지 널리 사용되고 있습니다.

하지만 한 단계의 변환만 하는 얕은 학습이기 때문에 공간 변환에 한계가 있습니다.



---

**Reference**

기계학습, 오일석, 2017, 비지도학습